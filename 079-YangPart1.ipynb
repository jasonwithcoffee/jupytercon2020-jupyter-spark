{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!scala -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,trim,udf,regexp_replace,lower,hash,countDistinct,array_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName('Covid19 News LDA Topics')\\\n",
    "  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \\\n",
    "  .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve COVID-19 News Data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"\"\"\n",
    "select distinct NET.REG_DOMAIN(URL) domain, Title, CONCAT(Title, \" \", Context) text\n",
    "from `gdelt-bq.covid19.onlinenews`\n",
    "where rand()<0.1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Query Results').getOrCreate()\n",
    "bq = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying BigQuery\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x7f740ad3b6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Querying BigQuery')\n",
    "table_id = \"data-analysis-202319.jy_covid19_analysis.test_tmp_table\"\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    allow_large_results=True, destination=table_id, use_legacy_sql=False\n",
    ")\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "\n",
    "query_job = bq.query(QUERY, job_config=job_config)\n",
    "query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('bigquery') \\\n",
    "    .option('dataset', query_job.destination.dataset_id) \\\n",
    "    .load(query_job.destination.table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_ignore(x):\n",
    "    return x.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "ascii_udf = udf(ascii_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(column):\n",
    "     return trim(lower(regexp_replace(column,'[^\\sa-zA-Z0-9]', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df.withColumn(\"text_no_ascii\", ascii_udf('text')) \\\n",
    ".withColumn(\"text_no_special\", removePunctuation(col(\"text_no_ascii\"))) \\\n",
    ".withColumn(\"text_lower\", lower(col(\"text_no_special\"))) \\\n",
    ".withColumn(\"text_hashed\", hash('text_lower')) \\\n",
    ".filter('length(text_lower) > 10') \n",
    "# .limit(500000)\n",
    "# limit to x0k records for testing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_unique = df_text.select(\"text_lower\",\"text_hashed\")\\\n",
    "    .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2807552"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2190020"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_unique.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the outputs of the last two cells, we see that a large amount of our news records are duplicates, or articles that were syndicated among news outlets. As a result, we drop the duplicates for our topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe with top domains\n",
    "df_domains_top = df_text\\\n",
    ".groupBy(\"domain\")\\\n",
    ".count()\\\n",
    ".sort(col(\"count\").desc()) \\\n",
    ".limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe with domain-text_hashed mapping for only top domains\n",
    "df_domain = df_text\\\n",
    ".join(df_domains_top, on=['domain'])\\\n",
    ".select(\"domain\",\"text_hashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Prepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://spark.apache.org/docs/latest/ml-features.html#tf-idf\n",
    "# https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3741049972324885/3783546674231782/4413065072037724/latest.html\n",
    "\n",
    "# remove \"\" from array https://sparkbyexamples.com/spark/working-with-spark-dataframe-filter/\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text_lower\", outputCol=\"words\")\n",
    "df_tokenizer = tokenizer.transform(df_text_unique)\n",
    "\n",
    "\n",
    "# Add Custom StopWords https://stackoverflow.com/questions/43623400/how-to-add-custom-stop-word-list-to-stopwordsremover\n",
    "stopwordList = [\"\", \"coronavirus\",\"covid19\",\"pandemic\", \"said\", \"also\", \"ap\", \"due\"] \n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stopwordList)\n",
    "df_remover = remover.transform(df_tokenizer)\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\",\n",
    "                             vocabSize=5000).fit(df_remover)\n",
    "\n",
    "df_text_out = vectorizer.transform(df_remover)\n",
    "\n",
    "# hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "\n",
    "# model = pipeline.fit(df_titles)\n",
    "# df_titles_out = model.transform(df_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>text_lower</th><th>text_hashed</th><th>words</th><th>filtered</th><th>features</th></tr>\n",
       "<tr><td>de blasio says ny...</td><td>1640177803</td><td>[de, blasio, says...</td><td>[de, blasio, says...</td><td>(5000,[1,6,9,13,1...</td></tr>\n",
       "<tr><td>rupee vs dollar r...</td><td>-1773633840</td><td>[rupee, vs, dolla...</td><td>[rupee, vs, dolla...</td><td>(5000,[0,2,5,15,2...</td></tr>\n",
       "<tr><td>baidus value will...</td><td>128485907</td><td>[baidus, value, w...</td><td>[baidus, value, u...</td><td>(5000,[3,10,20,27...</td></tr>\n",
       "<tr><td>south koreans ret...</td><td>1451117704</td><td>[south, koreans, ...</td><td>[south, koreans, ...</td><td>(5000,[4,8,9,10,2...</td></tr>\n",
       "<tr><td>stock markets plu...</td><td>516600583</td><td>[stock, markets, ...</td><td>[stock, markets, ...</td><td>(5000,[1,39,67,10...</td></tr>\n",
       "<tr><td>trading briefly h...</td><td>-98579109</td><td>[trading, briefly...</td><td>[trading, briefly...</td><td>(5000,[0,1,2,6,15...</td></tr>\n",
       "<tr><td>powell recovery m...</td><td>-1364474195</td><td>[powell, recovery...</td><td>[powell, recovery...</td><td>(5000,[5,10,74,11...</td></tr>\n",
       "<tr><td>coronavirus south...</td><td>-1350252603</td><td>[coronavirus, sou...</td><td>[south, attleboro...</td><td>(5000,[1,3,5,11,2...</td></tr>\n",
       "<tr><td>trump churches ar...</td><td>440719351</td><td>[trump, churches,...</td><td>[trump, churches,...</td><td>(5000,[23,24,38,5...</td></tr>\n",
       "<tr><td>families connect ...</td><td>2016617532</td><td>[families, connec...</td><td>[families, connec...</td><td>(5000,[3,16,37,39...</td></tr>\n",
       "<tr><td>demand for food p...</td><td>1371285788</td><td>[demand, for, foo...</td><td>[demand, food, pa...</td><td>(5000,[13,14,15,3...</td></tr>\n",
       "<tr><td>hope isnt a strat...</td><td>-1836386410</td><td>[hope, isnt, a, s...</td><td>[hope, isnt, stra...</td><td>(5000,[10,14,20,3...</td></tr>\n",
       "<tr><td>coronavirus first...</td><td>1585984920</td><td>[coronavirus, fir...</td><td>[first, patient, ...</td><td>(5000,[2,4,8,9,16...</td></tr>\n",
       "<tr><td>two more covid19 ...</td><td>1764934900</td><td>[two, more, covid...</td><td>[two, deaths, rep...</td><td>(5000,[0,1,3,7,11...</td></tr>\n",
       "<tr><td>how one city mayo...</td><td>-1153803323</td><td>[how, one, city, ...</td><td>[one, city, mayor...</td><td>(5000,[0,8,9,31,4...</td></tr>\n",
       "<tr><td>connecticut bans ...</td><td>-850812339</td><td>[connecticut, ban...</td><td>[connecticut, ban...</td><td>(5000,[3,4,10,16,...</td></tr>\n",
       "<tr><td>as trump urges re...</td><td>148682275</td><td>[as, trump, urges...</td><td>[trump, urges, re...</td><td>(5000,[0,3,30,36,...</td></tr>\n",
       "<tr><td>southern gardenin...</td><td>-315779542</td><td>[southern, garden...</td><td>[southern, garden...</td><td>(5000,[23,27,42,6...</td></tr>\n",
       "<tr><td>health experts sh...</td><td>-1095816646</td><td>[health, experts,...</td><td>[health, experts,...</td><td>(5000,[2,34,53,86...</td></tr>\n",
       "<tr><td>gops massie outra...</td><td>-164068980</td><td>[gops, massie, ou...</td><td>[gops, massie, ou...</td><td>(5000,[2,9,13,23,...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+-----------+--------------------+--------------------+--------------------+\n",
       "|          text_lower|text_hashed|               words|            filtered|            features|\n",
       "+--------------------+-----------+--------------------+--------------------+--------------------+\n",
       "|coronavirus april...|   58167264|[coronavirus, apr...|[april, 26, devel...|(5000,[0,2,6,8,13...|\n",
       "|bursa malaysia op...|  235224924|[bursa, malaysia,...|[bursa, malaysia,...|(5000,[0,5,9,13,2...|\n",
       "|with 1813 new cas...| -867641270|[with, 1813, new,...|[1813, new, cases...|(5000,[0,1,8,14,1...|\n",
       "|live covid19 upda...|-1778154083|[live, covid19, u...|[live, updates, i...|(5000,[0,1,2,12,1...|\n",
       "|coronavirus lates...| -652789240|[coronavirus, lat...|[latest, 37282, c...|(5000,[0,8,12,15,...|\n",
       "|washington county...|  632571573|[washington, coun...|[washington, coun...|(5000,[0,2,17,22,...|\n",
       "|us to screen airl...|-1759566662|[us, to, screen, ...|[us, screen, airl...|(5000,[0,1,3,5,9,...|\n",
       "|china reports fir...|-1197542869|[china, reports, ...|[china, reports, ...|(5000,[2,3,4,9,16...|\n",
       "|chris cuomo says ...|-1999933592|[chris, cuomo, sa...|[chris, cuomo, sa...|(5000,[0,17,29,36...|\n",
       "|governor expands ...| -193824755|[governor, expand...|[governor, expand...|(5000,[6,8,11,13,...|\n",
       "|covid19 developme...|  744445179|[covid19, develop...|[developments, ne...|(5000,[1,2,3,14,1...|\n",
       "|not real news a l...|  958568923|[not, real, news,...|[real, news, look...|(5000,[0,1,3,7,20...|\n",
       "|is takeout and de...|-1786409693|[is, takeout, and...|[takeout, deliver...|(5000,[0,4,10,17,...|\n",
       "|covid19 us pledge...| -773039242|[covid19, us, ple...|[us, pledges, sup...|(5000,[0,5,9,11,1...|\n",
       "|startup hits 1 bi...|  507553156|[startup, hits, 1...|[startup, hits, 1...|(5000,[0,1,2,3,4,...|\n",
       "|cardi b discloses...| 1958993239|[cardi, b, disclo...|[cardi, b, disclo...|(5000,[41,57,72,8...|\n",
       "|are we experienci...|  631026400|[are, we, experie...|[experiencing, ca...|(5000,[4,50,78,10...|\n",
       "|with met gala pos...| 1503786081|[with, met, gala,...|[met, gala, postp...|(5000,[10,44,68,7...|\n",
       "|what is bogging d...| -154716429|[what, is, boggin...|[bogging, philipp...|(5000,[4,14,18,37...|\n",
       "|what is a black s...|  854563442|[what, is, a, bla...|[black, swan, eve...|(5000,[0,10,58,99...|\n",
       "+--------------------+-----------+--------------------+--------------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/feynmanliang/3b6555758a27adcb527d\n",
    "# https://databricks.com/blog/2015/09/22/large-scale-topic-modeling-improvements-to-lda-on-apache-spark.html\n",
    "\n",
    "numTopics = 10\n",
    "lda = LDA(featuresCol = 'features', k=numTopics, maxIter =30, optimizer=\"online\")\n",
    "\n",
    "model = lda.fit(df_text_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ll = model.logLikelihood(df_titles_out)\n",
    "# lp = model.logPerplexity(df_titles_out)\n",
    "# print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "# print(\"The upper bound on perplexity: \" + str(lp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.estimatedDocConcentration()\n",
    "# model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe topics.\n",
    "# https://www.zstat.pl/2018/02/07/scala-spark-get-topics-words-from-lda-model/\n",
    "topics = model.describeTopics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DFindices_to_DFnames(topics, vectorizer, num_topics=10):\n",
    "    words = vectorizer.vocabulary\n",
    "    cSchema = StructType([StructField('termIndices',ArrayType(LongType(),True),True),\n",
    "                          StructField('termNames',ArrayType(StringType(),True),True)])\n",
    "    df_termNames = spark.createDataFrame(sc.emptyRDD(), schema=cSchema)\n",
    "    \n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        indices = topics.select(\"termIndices\").collect()[i][0]\n",
    "        wordsList = []\n",
    "        for j in indices:\n",
    "            wordsList.append(words[j])\n",
    "        input_list = [(indices,wordsList)]\n",
    "        df_i = spark.createDataFrame(input_list)\n",
    "        df_out = df_i.withColumnRenamed(\"_1\",\"termIndices\") \\\n",
    "            .withColumnRenamed(\"_2\",\"termNames\")\n",
    "\n",
    "        df_termNames = df_termNames.union(df_out)\n",
    "    return df_termNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------+---------------------+\n",
      "|topic|termNames                                  |termIndices          |\n",
      "+-----+-------------------------------------------+---------------------+\n",
      "|0    |[2020, school, new, time, students]        |[21, 140, 1, 27, 213]|\n",
      "|1    |[market, us, economic, global, economy]    |[95, 5, 104, 75, 111]|\n",
      "|2    |[cases, new, deaths, confirmed, number]    |[0, 1, 12, 17, 15]   |\n",
      "|3    |[positive, tested, news, home, people]     |[8, 25, 7, 19, 3]    |\n",
      "|4    |[health, county, state, cases, public]     |[2, 22, 6, 0, 13]    |\n",
      "|5    |[patients, hospital, health, care, medical]|[33, 57, 2, 46, 51]  |\n",
      "|6    |[people, virus, health, vaccine, spread]   |[3, 4, 2, 334, 11]   |\n",
      "|7    |[new, china, lockdown, people, travel]     |[1, 34, 14, 3, 96]   |\n",
      "|8    |[help, support, food, people, new]         |[39, 86, 132, 3, 1]  |\n",
      "|9    |[trump, us, people, president, health]     |[53, 5, 3, 59, 2]    |\n",
      "+-----+-------------------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_termNames = DFindices_to_DFnames(topics, vectorizer, num_topics=numTopics)\n",
    "topics_with_names = topics.join(df_termNames, on=['termIndices'])\n",
    "topics_with_names.select(\"topic\", \"termNames\", \"termIndices\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(termIndices,ArrayType(IntegerType,false),true),StructField(topic,IntegerType,false),StructField(termWeights,ArrayType(DoubleType,false),true),StructField(termNames,ArrayType(StringType,true),true)))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_with_names.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>text_lower</th><th>text_hashed</th><th>words</th><th>filtered</th><th>features</th><th>topicDistribution</th></tr>\n",
       "<tr><td>de blasio says ny...</td><td>1640177803</td><td>[de, blasio, says...</td><td>[de, blasio, says...</td><td>(5000,[1,6,9,13,1...</td><td>[0.33444016287311...</td></tr>\n",
       "<tr><td>rupee vs dollar r...</td><td>-1773633840</td><td>[rupee, vs, dolla...</td><td>[rupee, vs, dolla...</td><td>(5000,[0,2,5,15,2...</td><td>[0.00168033910113...</td></tr>\n",
       "<tr><td>baidus value will...</td><td>128485907</td><td>[baidus, value, w...</td><td>[baidus, value, u...</td><td>(5000,[3,10,20,27...</td><td>[0.00287827704010...</td></tr>\n",
       "<tr><td>south koreans ret...</td><td>1451117704</td><td>[south, koreans, ...</td><td>[south, koreans, ...</td><td>(5000,[4,8,9,10,2...</td><td>[0.60250762043066...</td></tr>\n",
       "<tr><td>stock markets plu...</td><td>516600583</td><td>[stock, markets, ...</td><td>[stock, markets, ...</td><td>(5000,[1,39,67,10...</td><td>[0.00156216042141...</td></tr>\n",
       "<tr><td>trading briefly h...</td><td>-98579109</td><td>[trading, briefly...</td><td>[trading, briefly...</td><td>(5000,[0,1,2,6,15...</td><td>[0.00164921744532...</td></tr>\n",
       "<tr><td>powell recovery m...</td><td>-1364474195</td><td>[powell, recovery...</td><td>[powell, recovery...</td><td>(5000,[5,10,74,11...</td><td>[0.00228583131459...</td></tr>\n",
       "<tr><td>coronavirus south...</td><td>-1350252603</td><td>[coronavirus, sou...</td><td>[south, attleboro...</td><td>(5000,[1,3,5,11,2...</td><td>[0.18304239081944...</td></tr>\n",
       "<tr><td>trump churches ar...</td><td>440719351</td><td>[trump, churches,...</td><td>[trump, churches,...</td><td>(5000,[23,24,38,5...</td><td>[0.00228575271392...</td></tr>\n",
       "<tr><td>families connect ...</td><td>2016617532</td><td>[families, connec...</td><td>[families, connec...</td><td>(5000,[3,16,37,39...</td><td>[0.78465695252273...</td></tr>\n",
       "<tr><td>demand for food p...</td><td>1371285788</td><td>[demand, for, foo...</td><td>[demand, food, pa...</td><td>(5000,[13,14,15,3...</td><td>[0.00164918907231...</td></tr>\n",
       "<tr><td>hope isnt a strat...</td><td>-1836386410</td><td>[hope, isnt, a, s...</td><td>[hope, isnt, stra...</td><td>(5000,[10,14,20,3...</td><td>[0.00217395336160...</td></tr>\n",
       "<tr><td>coronavirus first...</td><td>1585984920</td><td>[coronavirus, fir...</td><td>[first, patient, ...</td><td>(5000,[2,4,8,9,16...</td><td>[0.00193684790817...</td></tr>\n",
       "<tr><td>two more covid19 ...</td><td>1764934900</td><td>[two, more, covid...</td><td>[two, deaths, rep...</td><td>(5000,[0,1,3,7,11...</td><td>[0.00222843907320...</td></tr>\n",
       "<tr><td>how one city mayo...</td><td>-1153803323</td><td>[how, one, city, ...</td><td>[one, city, mayor...</td><td>(5000,[0,8,9,31,4...</td><td>[0.00202514088518...</td></tr>\n",
       "<tr><td>connecticut bans ...</td><td>-850812339</td><td>[connecticut, ban...</td><td>[connecticut, ban...</td><td>(5000,[3,4,10,16,...</td><td>[0.00145953641518...</td></tr>\n",
       "<tr><td>as trump urges re...</td><td>148682275</td><td>[as, trump, urges...</td><td>[trump, urges, re...</td><td>(5000,[0,3,30,36,...</td><td>[0.00262326455162...</td></tr>\n",
       "<tr><td>southern gardenin...</td><td>-315779542</td><td>[southern, garden...</td><td>[southern, garden...</td><td>(5000,[23,27,42,6...</td><td>[0.23264774486769...</td></tr>\n",
       "<tr><td>health experts sh...</td><td>-1095816646</td><td>[health, experts,...</td><td>[health, experts,...</td><td>(5000,[2,34,53,86...</td><td>[0.24704162947906...</td></tr>\n",
       "<tr><td>gops massie outra...</td><td>-164068980</td><td>[gops, massie, ou...</td><td>[gops, massie, ou...</td><td>(5000,[2,9,13,23,...</td><td>[0.24808460458319...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
       "|          text_lower|text_hashed|               words|            filtered|            features|   topicDistribution|\n",
       "+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
       "|de blasio says ny...| 1640177803|[de, blasio, says...|[de, blasio, says...|(5000,[1,6,9,13,1...|[0.33444016287311...|\n",
       "|rupee vs dollar r...|-1773633840|[rupee, vs, dolla...|[rupee, vs, dolla...|(5000,[0,2,5,15,2...|[0.00168033910113...|\n",
       "|baidus value will...|  128485907|[baidus, value, w...|[baidus, value, u...|(5000,[3,10,20,27...|[0.00287827704010...|\n",
       "|south koreans ret...| 1451117704|[south, koreans, ...|[south, koreans, ...|(5000,[4,8,9,10,2...|[0.60250762043066...|\n",
       "|stock markets plu...|  516600583|[stock, markets, ...|[stock, markets, ...|(5000,[1,39,67,10...|[0.00156216042141...|\n",
       "|trading briefly h...|  -98579109|[trading, briefly...|[trading, briefly...|(5000,[0,1,2,6,15...|[0.00164921744532...|\n",
       "|powell recovery m...|-1364474195|[powell, recovery...|[powell, recovery...|(5000,[5,10,74,11...|[0.00228583131459...|\n",
       "|coronavirus south...|-1350252603|[coronavirus, sou...|[south, attleboro...|(5000,[1,3,5,11,2...|[0.18304239081944...|\n",
       "|trump churches ar...|  440719351|[trump, churches,...|[trump, churches,...|(5000,[23,24,38,5...|[0.00228575271392...|\n",
       "|families connect ...| 2016617532|[families, connec...|[families, connec...|(5000,[3,16,37,39...|[0.78465695252273...|\n",
       "|demand for food p...| 1371285788|[demand, for, foo...|[demand, food, pa...|(5000,[13,14,15,3...|[0.00164918907231...|\n",
       "|hope isnt a strat...|-1836386410|[hope, isnt, a, s...|[hope, isnt, stra...|(5000,[10,14,20,3...|[0.00217395336160...|\n",
       "|coronavirus first...| 1585984920|[coronavirus, fir...|[first, patient, ...|(5000,[2,4,8,9,16...|[0.00193684790817...|\n",
       "|two more covid19 ...| 1764934900|[two, more, covid...|[two, deaths, rep...|(5000,[0,1,3,7,11...|[0.00222843907320...|\n",
       "|how one city mayo...|-1153803323|[how, one, city, ...|[one, city, mayor...|(5000,[0,8,9,31,4...|[0.00202514088518...|\n",
       "|connecticut bans ...| -850812339|[connecticut, ban...|[connecticut, ban...|(5000,[3,4,10,16,...|[0.00145953641518...|\n",
       "|as trump urges re...|  148682275|[as, trump, urges...|[trump, urges, re...|(5000,[0,3,30,36,...|[0.00262326455162...|\n",
       "|southern gardenin...| -315779542|[southern, garden...|[southern, garden...|(5000,[23,27,42,6...|[0.23264774486769...|\n",
       "|health experts sh...|-1095816646|[health, experts,...|[health, experts,...|(5000,[2,34,53,86...|[0.24704162947906...|\n",
       "|gops massie outra...| -164068980|[gops, massie, ou...|[gops, massie, ou...|(5000,[2,9,13,23,...|[0.24808460458319...|\n",
       "+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the result\n",
    "transformed = model.transform(df_text_out)\n",
    "transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/38384347/how-to-split-vector-into-columns-using-pyspark\n",
    "def to_array(col):\n",
    "    def to_array_(v):\n",
    "        return v.toArray().tolist()\n",
    "    # Important: asNondeterministic requires Spark 2.3 or later\n",
    "    # It can be safely removed i.e.\n",
    "    # return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "    # but at the cost of decreased performance\n",
    "    return udf(to_array_, ArrayType(DoubleType())).asNondeterministic()(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = transformed \\\n",
    ".withColumn(\"topic_array\", to_array(\"topicDistribution\")) \\\n",
    ".withColumn(\"topic_max\", array_max(\"topic_array\")) \n",
    "# .withColumn(\"topic_index\", array_position(col(\"topic_array\"), \"topic_max\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF to assign index to list items \n",
    "# https://html.developreference.com/article/10897735/Get+index+of+item+in+array+that+is+a+column+in+a+Spark+dataframe\n",
    "max_topic_index = udf(lambda x,y: [i for i, e in enumerate(x) if e==y ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed2 = df_transformed \\\n",
    ".withColumn(\"topicPred\", max_topic_index(col(\"topic_array\"),col(\"topic_max\")))\\\n",
    ".withColumn(\"topicPredStr\",col(\"topicPred\").substr(2,1))\\\n",
    ".withColumn(\"topic\", col(\"topicPredStr\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed_names = topics_with_names.join(df_transformed2, on=['topic'])\\\n",
    ".select(\"topic\", \"topic_max\", \"termNames\",\"text_hashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------+------+\n",
      "|topic|termNames                                  |count |\n",
      "+-----+-------------------------------------------+------+\n",
      "|0    |[2020, school, new, time, students]        |281955|\n",
      "|1    |[market, us, economic, global, economy]    |252257|\n",
      "|2    |[cases, new, deaths, confirmed, number]    |253840|\n",
      "|3    |[positive, tested, news, home, people]     |158069|\n",
      "|4    |[health, county, state, cases, public]     |257269|\n",
      "|5    |[patients, hospital, health, care, medical]|157643|\n",
      "|6    |[people, virus, health, vaccine, spread]   |207877|\n",
      "|7    |[new, china, lockdown, people, travel]     |225033|\n",
      "|8    |[help, support, food, people, new]         |254329|\n",
      "|9    |[trump, us, people, president, health]     |141748|\n",
      "+-----+-------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed_names_agg = df_transformed_names\\\n",
    "    .groupBy(\"topic\", \"termNames\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"topic\").cache()\n",
    "\n",
    "df_transformed_names_agg.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamp for saving table records\n",
    "save_time = datetime.now().strftime(\"%Y%m%d_%H_%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed_names_agg.select(\"topic\", \n",
    "                                col(\"termNames\").cast(StringType()).alias(\"termNamesStr\"),\n",
    "                                \"count\")\\\n",
    "    .write.format('bigquery') \\\n",
    "    .option(\"temporaryGcsBucket\",\"data-analysis-jy\") \\\n",
    "    .save('data-analysis-202319.jy_covid19_analysis.topic_counts_'+str(save_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics by Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed_domains = df_transformed_names.join(df_domain, on=['text_hashed'])\\\n",
    "    .groupBy(\"topic\",\"termNames\", \"domain\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"topic\",\"domain\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------+---------------------+-----+\n",
      "|topic|termNames                          |domain               |count|\n",
      "+-----+-----------------------------------+---------------------+-----+\n",
      "|0    |[2020, school, new, time, students]|680news.com          |367  |\n",
      "|0    |[2020, school, new, time, students]|abc.net.au           |485  |\n",
      "|0    |[2020, school, new, time, students]|abs-cbn.com          |225  |\n",
      "|0    |[2020, school, new, time, students]|accesswdun.com       |354  |\n",
      "|0    |[2020, school, new, time, students]|aljazeera.com        |204  |\n",
      "|0    |[2020, school, new, time, students]|allafrica.com        |803  |\n",
      "|0    |[2020, school, new, time, students]|aninews.in           |214  |\n",
      "|0    |[2020, school, new, time, students]|apnews.com           |338  |\n",
      "|0    |[2020, school, new, time, students]|bbc.co.uk            |466  |\n",
      "|0    |[2020, school, new, time, students]|bbc.com              |245  |\n",
      "|0    |[2020, school, new, time, students]|business-standard.com|831  |\n",
      "|0    |[2020, school, new, time, students]|cbc.ca               |679  |\n",
      "|0    |[2020, school, new, time, students]|channelnewsasia.com  |214  |\n",
      "|0    |[2020, school, new, time, students]|china.org.cn         |100  |\n",
      "|0    |[2020, school, new, time, students]|chinadaily.com.cn    |300  |\n",
      "|0    |[2020, school, new, time, students]|chron.com            |1255 |\n",
      "|0    |[2020, school, new, time, students]|cnbc.com             |192  |\n",
      "|0    |[2020, school, new, time, students]|cnn.com              |388  |\n",
      "|0    |[2020, school, new, time, students]|ctpost.com           |726  |\n",
      "|0    |[2020, school, new, time, students]|ctvnews.ca           |652  |\n",
      "|0    |[2020, school, new, time, students]|daijiworld.com       |534  |\n",
      "|0    |[2020, school, new, time, students]|dailymail.co.uk      |3180 |\n",
      "|0    |[2020, school, new, time, students]|dawn.com             |346  |\n",
      "|0    |[2020, school, new, time, students]|eveningexpress.co.uk |370  |\n",
      "|0    |[2020, school, new, time, students]|express.co.uk        |528  |\n",
      "|0    |[2020, school, new, time, students]|financialexpress.com |169  |\n",
      "|0    |[2020, school, new, time, students]|firstpost.com        |460  |\n",
      "|0    |[2020, school, new, time, students]|fool.com             |250  |\n",
      "|0    |[2020, school, new, time, students]|forbes.com           |864  |\n",
      "|0    |[2020, school, new, time, students]|foxnews.com          |749  |\n",
      "+-----+-----------------------------------+---------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed_domains.show(30,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed_domains.select(\"topic\", \n",
    "                              col(\"termNames\").cast(StringType()).alias(\"termNamesStr\"),\n",
    "                              \"domain\",\n",
    "                              \"count\")\\\n",
    "    .write.format('bigquery') \\\n",
    "    .option(\"temporaryGcsBucket\",\"data-analysis-jy\") \\\n",
    "    .save('data-analysis-202319.jy_covid19_analysis.topic_domain_counts_'+str(save_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"gs://data-analysis-jy/covid19LDA/lda-model-20200719\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df_transformed_domains.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas csv\n",
    "path = \"gs://data-analysis-jy/covid19LDA/pandas_df.csv\"\n",
    "\n",
    "pdf.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
